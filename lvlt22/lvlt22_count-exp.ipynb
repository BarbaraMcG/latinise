{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ca6207f0-062b-41fd-afe5-e144af88339f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: rpy2 in /media/HOME_DATA/miniconda3/envs/lvlt22/lib/python3.10/site-packages (3.5.3)\n",
      "Requirement already satisfied: cffi>=1.10.0 in /media/HOME_DATA/miniconda3/envs/lvlt22/lib/python3.10/site-packages (from rpy2) (1.15.1)\n",
      "Requirement already satisfied: pytz in /media/HOME_DATA/miniconda3/envs/lvlt22/lib/python3.10/site-packages (from rpy2) (2022.1)\n",
      "Requirement already satisfied: tzlocal in /media/HOME_DATA/miniconda3/envs/lvlt22/lib/python3.10/site-packages (from rpy2) (4.2)\n",
      "Requirement already satisfied: jinja2 in /media/HOME_DATA/miniconda3/envs/lvlt22/lib/python3.10/site-packages (from rpy2) (3.1.2)\n",
      "Requirement already satisfied: pycparser in /media/HOME_DATA/miniconda3/envs/lvlt22/lib/python3.10/site-packages (from cffi>=1.10.0->rpy2) (2.21)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /media/HOME_DATA/miniconda3/envs/lvlt22/lib/python3.10/site-packages (from jinja2->rpy2) (2.1.1)\n",
      "Requirement already satisfied: pytz-deprecation-shim in /media/HOME_DATA/miniconda3/envs/lvlt22/lib/python3.10/site-packages (from tzlocal->rpy2) (0.1.0.post0)\n",
      "Requirement already satisfied: tzdata in /media/HOME_DATA/miniconda3/envs/lvlt22/lib/python3.10/site-packages (from pytz-deprecation-shim->tzlocal->rpy2) (2022.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install rpy2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6ce0818e-6f22-4b36-bcde-524c4ff170c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import time\n",
    "from nltk.util import skipgrams\n",
    "from nltk.lm import NgramCounter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "52afaceb-dfcc-49eb-83a4-554e64f3ced2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "R[write to console]: Instalowanie pakietu w ‘/home/krzys/R/x86_64-pc-linux-gnu-library/4.1’\n",
      "(ponieważ ‘lib’ nie jest określony)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"/home/krzys/R/x86_64-pc-linux-gnu-library/4.1\"\n",
      "[2] \"/usr/local/lib/R/site-library\"                \n",
      "[3] \"/usr/lib/R/site-library\"                      \n",
      "[4] \"/usr/lib/R/library\"                           \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "R[write to console]: próbowanie adresu URL 'https://cloud.r-project.org/src/contrib/wordspace_0.2-8.tar.gz'\n",
      "\n",
      "R[write to console]: Content type 'application/x-gzip'\n",
      "R[write to console]:  length 1839490 bytes (1.8 MB)\n",
      "\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: \n",
      "\n",
      "R[write to console]: downloaded 1.8 MB\n",
      "\n",
      "\n",
      "* installing *source* package ‘wordspace’ ...\n",
      "** package ‘wordspace’ successfully unpacked and MD5 sums checked\n",
      "** using staged installation\n",
      "** libs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "g++ -std=gnu++14 -I\"/usr/share/R/include\" -DNDEBUG  -I'/home/krzys/R/x86_64-pc-linux-gnu-library/4.1/Rcpp/include'   -fopenmp -fpic  -g -O2 -ffile-prefix-map=/build/r-base-4A2Reg/r-base-4.1.2=. -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -g  -c RcppExports.cpp -o RcppExports.o\n",
      "g++ -std=gnu++14 -I\"/usr/share/R/include\" -DNDEBUG  -I'/home/krzys/R/x86_64-pc-linux-gnu-library/4.1/Rcpp/include'   -fopenmp -fpic  -g -O2 -ffile-prefix-map=/build/r-base-4A2Reg/r-base-4.1.2=. -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -g  -c wordspace.cpp -o wordspace.o\n",
      "g++ -std=gnu++14 -shared -L/usr/lib/R/lib -Wl,-Bsymbolic-functions -flto=auto -ffat-lto-objects -flto=auto -Wl,-z,relro -o wordspace.so RcppExports.o wordspace.o -fopenmp -L/usr/lib/R/lib -lR\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "installing to /home/krzys/R/x86_64-pc-linux-gnu-library/4.1/00LOCK-wordspace/00new/wordspace/libs\n",
      "** R\n",
      "** data\n",
      "*** moving datasets to lazyload DB\n",
      "** inst\n",
      "** byte-compile and prepare package for lazy loading\n",
      "** help\n",
      "*** installing help indices\n",
      "** building package indices\n",
      "** installing vignettes\n",
      "** testing if installed package can be loaded from temporary location\n",
      "** checking absolute paths in shared objects and dynamic libraries\n",
      "** testing if installed package can be loaded from final location\n",
      "** testing if installed package keeps a record of temporary installation path\n",
      "* DONE (wordspace)\n",
      "R[write to console]: \n",
      "\n",
      "R[write to console]: \n",
      "R[write to console]: The downloaded source packages are in\n",
      "\t‘/tmp/RtmpyiFL1E/downloaded_packages’\n",
      "R[write to console]: \n",
      "R[write to console]: \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import rpy2.robjects as robjects\n",
    "from rpy2.robjects.packages import importr\n",
    "import rpy2.robjects.packages as rpackages\n",
    "from rpy2.robjects import StrVector, IntVector\n",
    "utils = rpackages.importr('utils')\n",
    "utils.chooseCRANmirror(ind=1) # select the first mirror in the list\n",
    "r_libs = '/home/krzys/R/x86_64-pc-linux-gnu-library/4.1'# R libs\n",
    "base = importr('base')\n",
    "print(base._libPaths())\n",
    "utils.install_packages(\"wordspace\")\n",
    "wordspace = importr(\"wordspace\", lib_loc=r_libs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a8fd273c-74ec-4992-8d5c-23f107da68fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define terms we're interested in\n",
    "socio_political_terms = [\"civitas\", \"consilium\", \"consul\", \"dux\", \"gens\", \"hostis\", \"imperator\", \"jus\", \"labor\", \"natio\", \"nobilitas\", \"pontifex\", \"pontificium\", \"populus\", \"potestas\", \"regnum\", \"senatus\", \"sodes\", \"urbs\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcb91254-b747-4255-b29e-43b406aa1ceb",
   "metadata": {
    "tags": []
   },
   "source": [
    "## The corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "798091c9-f5ca-4a34-86be-4f1ef8b6a1e5",
   "metadata": {},
   "source": [
    "The corpus processing phase follows as close as possible BMG's workflow to keep models compatible. There are 2 exceptions:\n",
    "\n",
    "- all lemmas are converted to lowercase and\n",
    "- anomalous lemmas (mostly punctuation) are added to stopword list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "70e10132-796b-4986-95cd-03b61e5e1584",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare the corpus\n",
    "punctuation = ['.', ',', '...', ';', ':', '?', '(', ')', '-', '!', '[', ']', '\"', \"'\", '\"\"', '\\n']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ef29abf4-6559-4cbb-8a9c-1ed1306e92f0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# corpus files\n",
    "#dir_in = os.path.join(\"/home/krzys/Kod/streamlit/voces/data/corpora/latinise_IT_lemmas/\")\n",
    "dir_input =  os.path.join(\"/home/krzys/Kod/lvlt22/BMG/LatinISE_1/\") # includes texts first omitted due to parsing issues\n",
    "dir_in = os.path.join(dir_input, \"preprocessed_lemmas\")\n",
    "dir_in_words = os.path.join(dir_input, \"preprocessed_tokens\")\n",
    "files = os.listdir(os.path.join(dir_in))\n",
    "files = [f for f in files[:] if \"IT\" in f]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3b4fc7b-cd55-4e70-835c-9eb55281ef33",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Process the metadata\n",
    "We'll be storing corpus metadata in a data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eedfd89b-d475-423a-b55d-da420881ae6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# metadata (BMG)\n",
    "metadata_df = pd.read_csv(os.path.join(dir_input, 'latinise_metadata.csv'), sep = \",\")\n",
    "metadata_df = metadata_df[metadata_df['id'].str.startswith(\"IT\")]\n",
    "metadata_df.head()\n",
    "metadata_df[\"date\"] = metadata_df[\"date\"].astype('int') #ensure we're working with integers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "88dfb09d-4b13-4017-af49-1021535ad3d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "first_date = min(metadata_df.date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5945c95d-3d51-420a-9201-3236fa014831",
   "metadata": {},
   "outputs": [],
   "source": [
    "last_date = 900 # BMG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88584dca-5ed8-4244-bfea-3d7360ee974a",
   "metadata": {},
   "source": [
    "Define size of the time intervals:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c960fe9f-ef1b-43a8-97d4-23ff4f2fb201",
   "metadata": {},
   "outputs": [],
   "source": [
    "size_interval = 450 # BMG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c88fd7ce-6628-4f2f-ae4d-6d4dfab264d7",
   "metadata": {},
   "source": [
    "So there are"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8b9b391d-c3c3-42d3-9f62-4aa6d8a7e40e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_intervals = round((last_date-first_date)/size_interval) # BMG\n",
    "n_intervals"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0433ce17-8b31-4eba-8312-6cb004a18d1b",
   "metadata": {},
   "source": [
    "time intervals."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56591e2c-013a-4204-9e8c-eb31ad2334ae",
   "metadata": {
    "id": "bUW2Qo0e8rLF"
   },
   "source": [
    "Define the time periods and split the corpus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "18d2dfee-9059-4013-9e84-f1dc4ddc68a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-450, 0, 450, 900]\n",
      "['-450-0', '0-450', '450-900']\n"
     ]
    }
   ],
   "source": [
    "intervals = [None]*(n_intervals+1) # BMG\n",
    "for t in range(n_intervals+1):\n",
    "    #print(t)\n",
    "    if t == 0:\n",
    "        intervals[t] = first_date\n",
    "    else:\n",
    "        intervals[t] = intervals[t-1]+size_interval\n",
    "    #print(intervals[t])\n",
    "    \n",
    "print(intervals)\n",
    "periods_labels = [ str(p1) + '-' + str(p2) for p1, p2 in zip(intervals, intervals[1:]) ]\n",
    "print(periods_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94f46038-08bd-4299-9697-dd28a468e64a",
   "metadata": {},
   "source": [
    "Add a column to the metadata_df for the time interval:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0620e232-b2ba-4b7d-8c55-7927fc820b13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "range(-450, 0)\n",
      "19      -9\n",
      "34     -49\n",
      "39     -45\n",
      "42     -49\n",
      "57     -80\n",
      "      ... \n",
      "635   -149\n",
      "638   -107\n",
      "642    -37\n",
      "643    -37\n",
      "649   -229\n",
      "Name: date, Length: 77, dtype: int64\n",
      "1\n",
      "range(0, 450)\n",
      "18     382\n",
      "23     399\n",
      "24     391\n",
      "37     158\n",
      "38      49\n",
      "      ... \n",
      "682    382\n",
      "683    116\n",
      "684    116\n",
      "685    116\n",
      "686    116\n",
      "Name: date, Length: 235, dtype: int64\n",
      "2\n",
      "range(450, 900)\n",
      "20      524\n",
      "102     800\n",
      "104     800\n",
      "105     800\n",
      "106     800\n",
      "       ... \n",
      "609     598\n",
      "634     550\n",
      "636     550\n",
      "645     450\n",
      "1265    533\n",
      "Name: date, Length: 73, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>creator</th>\n",
       "      <th>date</th>\n",
       "      <th>type</th>\n",
       "      <th>file</th>\n",
       "      <th>time_interval</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>IT-LAT0001</td>\n",
       "      <td>Vulgata</td>\n",
       "      <td>Hieronymus</td>\n",
       "      <td>382</td>\n",
       "      <td>poetry</td>\n",
       "      <td>lat_0382_IT-LAT0001.txt</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>IT-LAT0537</td>\n",
       "      <td>Ars amatoria</td>\n",
       "      <td>Ovidius Naso, Publius</td>\n",
       "      <td>-9</td>\n",
       "      <td>poetry</td>\n",
       "      <td>lat_-009_IT-LAT0537.txt</td>\n",
       "      <td>-450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>IT-LAT0011</td>\n",
       "      <td>S. Benedicti Regula</td>\n",
       "      <td>Benedictus Nursianus</td>\n",
       "      <td>524</td>\n",
       "      <td>prose</td>\n",
       "      <td>lat_0524_IT-LAT0011.txt</td>\n",
       "      <td>450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>IT-LAT0012</td>\n",
       "      <td>In psalmis Davidis expositio</td>\n",
       "      <td>Thomas Aquinas: Sanctus</td>\n",
       "      <td>1254</td>\n",
       "      <td>prose</td>\n",
       "      <td>lat_1254_IT-LAT0012.txt</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>IT-LAT0014</td>\n",
       "      <td>Adoro te devote</td>\n",
       "      <td>Thomas Aquinas: Sanctus</td>\n",
       "      <td>1254</td>\n",
       "      <td>poetry</td>\n",
       "      <td>lat_1254_IT-LAT0014.txt</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>683</th>\n",
       "      <td>IT-LAT0534_1</td>\n",
       "      <td>De origine et situ Germanorum</td>\n",
       "      <td>Tacitus, Publius (Gaius) Cornelius</td>\n",
       "      <td>116</td>\n",
       "      <td>prose</td>\n",
       "      <td>lat_0116_IT-LAT0534_1.txt</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>684</th>\n",
       "      <td>IT-LAT0534_2</td>\n",
       "      <td>De vita Iulii Agricolae</td>\n",
       "      <td>Tacitus, Publius (Gaius) Cornelius</td>\n",
       "      <td>116</td>\n",
       "      <td>prose</td>\n",
       "      <td>lat_0116_IT-LAT0534_2.txt</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>685</th>\n",
       "      <td>IT-LAT0534_3</td>\n",
       "      <td>Dialogus de oratoribus</td>\n",
       "      <td>Tacitus, Publius (Gaius) Cornelius</td>\n",
       "      <td>116</td>\n",
       "      <td>prose</td>\n",
       "      <td>lat_0116_IT-LAT0534_3.txt</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>686</th>\n",
       "      <td>IT-LAT0534_4</td>\n",
       "      <td>Historiae</td>\n",
       "      <td>Tacitus, Publius (Gaius) Cornelius</td>\n",
       "      <td>116</td>\n",
       "      <td>prose</td>\n",
       "      <td>lat_0116_IT-LAT0534_4.txt</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1265</th>\n",
       "      <td>IT-LAT0202</td>\n",
       "      <td>Institutiones</td>\n",
       "      <td>Iustinianus, Caesar Flavius (Imperator Iustini...</td>\n",
       "      <td>533</td>\n",
       "      <td>prose</td>\n",
       "      <td>lat_0533_IT-LAT0202.txt</td>\n",
       "      <td>450</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>670 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                id                          title  \\\n",
       "18      IT-LAT0001                        Vulgata   \n",
       "19      IT-LAT0537                   Ars amatoria   \n",
       "20      IT-LAT0011            S. Benedicti Regula   \n",
       "21      IT-LAT0012   In psalmis Davidis expositio   \n",
       "22      IT-LAT0014                Adoro te devote   \n",
       "...            ...                            ...   \n",
       "683   IT-LAT0534_1  De origine et situ Germanorum   \n",
       "684   IT-LAT0534_2        De vita Iulii Agricolae   \n",
       "685   IT-LAT0534_3         Dialogus de oratoribus   \n",
       "686   IT-LAT0534_4                      Historiae   \n",
       "1265    IT-LAT0202                  Institutiones   \n",
       "\n",
       "                                                creator  date    type  \\\n",
       "18                                           Hieronymus   382  poetry   \n",
       "19                                Ovidius Naso, Publius    -9  poetry   \n",
       "20                                 Benedictus Nursianus   524   prose   \n",
       "21                              Thomas Aquinas: Sanctus  1254   prose   \n",
       "22                              Thomas Aquinas: Sanctus  1254  poetry   \n",
       "...                                                 ...   ...     ...   \n",
       "683                  Tacitus, Publius (Gaius) Cornelius   116   prose   \n",
       "684                  Tacitus, Publius (Gaius) Cornelius   116   prose   \n",
       "685                  Tacitus, Publius (Gaius) Cornelius   116   prose   \n",
       "686                  Tacitus, Publius (Gaius) Cornelius   116   prose   \n",
       "1265  Iustinianus, Caesar Flavius (Imperator Iustini...   533   prose   \n",
       "\n",
       "                           file time_interval  \n",
       "18      lat_0382_IT-LAT0001.txt             0  \n",
       "19      lat_-009_IT-LAT0537.txt          -450  \n",
       "20      lat_0524_IT-LAT0011.txt           450  \n",
       "21      lat_1254_IT-LAT0012.txt                \n",
       "22      lat_1254_IT-LAT0014.txt                \n",
       "...                         ...           ...  \n",
       "683   lat_0116_IT-LAT0534_1.txt             0  \n",
       "684   lat_0116_IT-LAT0534_2.txt             0  \n",
       "685   lat_0116_IT-LAT0534_3.txt             0  \n",
       "686   lat_0116_IT-LAT0534_4.txt             0  \n",
       "1265    lat_0533_IT-LAT0202.txt           450  \n",
       "\n",
       "[670 rows x 7 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metadata_df['time_interval'] = \"\"\n",
    "for t in range(len(intervals)-1):\n",
    "    print(t)\n",
    "    print(range(intervals[t],intervals[t+1]))\n",
    "    metadata_df_t = metadata_df.loc[metadata_df['date'].isin(range(intervals[t],intervals[t+1]))]\n",
    "    print(metadata_df_t.date)\n",
    "    metadata_df.loc[metadata_df['date'].isin(range(intervals[t],intervals[t+1])),'time_interval'] = intervals[t]\n",
    "metadata_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "71afdce0-52b0-40ac-873c-c62aafed72b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_dates(sign, date0):\n",
    "\n",
    "    if sign == \"0\":\n",
    "        if date0 == 0:\n",
    "            final_date = \"+0000\"\n",
    "        elif date0 < 100:\n",
    "            final_date = \"+\" + \"00\" + str(date0)\n",
    "            #print(\"1-final_date\", final_date)\n",
    "        elif date0 < 1000:\n",
    "            final_date = \"+\" + \"0\" + str(date0)\n",
    "            #print(\"2-final_date\", final_date)\n",
    "        else:\n",
    "            final_date = \"+\" + str(date0)\n",
    "            #print(\"3-final_date\", final_date)\n",
    "    else:\n",
    "        if date0 == 0:\n",
    "            final_date = \"+0000\"\n",
    "        elif date0 < 100:\n",
    "            final_date = str(sign) + \"00\" + str(date0)\n",
    "            #print(\"1-final_date\", final_date)\n",
    "        elif date0 < 1000:\n",
    "            final_date = str(sign) + \"0\" + str(date0)\n",
    "            #print(\"2-final_date\", final_date)\n",
    "        else:\n",
    "            final_date = str(sign) + str(date0)\n",
    "            #print(\"3-final_date\", final_date)\n",
    "\n",
    "    if final_date.startswith(\"+\"):\n",
    "        final_date = final_date.replace(\"+\", \"\")\n",
    "    return final_date"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edbf3407-0e34-4b81-a20a-78724fc6371f",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Read in corpus files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cde88461-5921-4a99-a85f-00087ba4cb10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare the corpus\n",
    "punctuation = ['.', ',', '...', ';', ':', '?', '(', ')', '-', '!', '[', ']', '\"', \"'\", '\"\"', '\\n', '']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cb5eeac0-7144-48c0-994a-020f0e19bd93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define corpus subset\n",
    "corpus_subset = metadata_df[metadata_df['date'] <= last_date].copy().reset_index(drop=True)\n",
    "filenames_subset = corpus_subset['file'] # filenames were defined above to get IT files only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ad1b80bd-f322-4cf9-8703-7e73e0b66b8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus.reader.plaintext import PlaintextCorpusReader, CategorizedPlaintextCorpusReader\n",
    "from nltk.tokenize.simple import SpaceTokenizer, LineTokenizer\n",
    "from nltk.text import Text, TextCollection\n",
    "class NltkCorpusFromDirNew(PlaintextCorpusReader):\n",
    "    \"A subclass of NLTK PlaintextCorpusReader\"\n",
    "    \n",
    "    word_tokenizer=SpaceTokenizer() # tokenize on whitespace\n",
    "    sent_tokenizer=LineTokenizer() # assume sentence per line\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        root,\n",
    "        fileids,\n",
    "        encoding=\"utf8\",        \n",
    "        word_tokenizer=word_tokenizer,\n",
    "        sent_tokenizer=sent_tokenizer,\n",
    "        tolower=False, punctuation=None\n",
    "    ):\n",
    "\n",
    "        PlaintextCorpusReader.__init__(self, root=root, fileids=fileids, encoding=encoding,\n",
    "                                       word_tokenizer=word_tokenizer,\n",
    "                                       sent_tokenizer=sent_tokenizer)\n",
    "        self.tolower = tolower\n",
    "        self.punctuation = punctuation\n",
    "        \n",
    "    def _read_word_block(self, stream):\n",
    "        words = []\n",
    "        for i in range(20):  # Read 20 lines at a time.\n",
    "            if self.punctuation is not None:\n",
    "                words.extend( [ token.lower() if self.tolower == True else token for token \n",
    "                               in self._word_tokenizer.tokenize(stream.readline()) \n",
    "                               if token not in self.punctuation and token != '' \n",
    "                              ])\n",
    "            else:\n",
    "                words.extend( [ token.lower() if self.tolower == True else token for token in self._word_tokenizer.tokenize(stream.readline()) ])\n",
    "        return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a29b39c0-c006-430d-be38-0dac52e95fac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This corpus contains  385  documents.\n"
     ]
    }
   ],
   "source": [
    "#prepare the corpus\n",
    "latinise = NltkCorpusFromDirNew(root=dir_in, fileids=filenames_subset,\n",
    "                                punctuation=punctuation, tolower=True)\n",
    "latinise_docs = []\n",
    "for fileid in latinise.fileids():\n",
    "    latinise_docs.append(Text(latinise.words(fileid)))\n",
    "print(\"This corpus contains \", len(latinise_docs), \" documents.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bc80590f-d798-4b7a-a42b-08b3d204724f",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = latinise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30c57120-3d84-4844-8866-f6b1b5b76389",
   "metadata": {},
   "source": [
    "In `rebuild` mode we'll compile DSMs using Stephanie Evert's `R` package `wordspace`. If not otherwise stated, all terms used come from [the package's documentation](https://cran.r-project.org/web/packages/wordspace/wordspace.pdf)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b1a80c0c-786d-46b0-ae90-50d273a26b79",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'rebuild' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_176382/3569112992.py\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mif\u001b[0m \u001b[0mrebuild\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0;31m#Let's compute cooccurrence counts in the corpus.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mwindow\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mskip\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m \u001b[0;31m# defines distance if skipgram\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mngrams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m \u001b[0mskipgrams\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwindow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskip\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msents\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'rebuild' is not defined"
     ]
    }
   ],
   "source": [
    "if rebuild == True:\n",
    "    #Let's compute cooccurrence counts in the corpus.\n",
    "    window=2\n",
    "    skip=5 # defines distance if skipgram\n",
    "    ngrams = [ skipgrams(sent, window, skip) for sent in corpus.sents() ]\n",
    "    ngram_counts = NgramCounter(ngrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a1b5fff-ceca-4e50-8330-345f2f375c5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get triples (target, feature, freq) from the ngram_counts\n",
    "if rebuild == True:\n",
    "    coocs = []\n",
    "    for node, freqs in ngram_counts[window].items(): # dict_items([(('ars',), FreqDist({'et': 149, 'sum': 129, ...}}))])\n",
    "        #print(word)\n",
    "        for cooc, freq  in freqs.items():\n",
    "            triple = (node[0], cooc, freq) # node term, cooc, freq\n",
    "            coocs.append(triple)\n",
    "    print(coocs[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33f3838e-af0b-4e4b-a17d-c1968237c1b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert cooc counts to matrix\n",
    "coocs_df = pd.DataFrame(coocs, columns=[\"target\", \"feature\", \"score\"])\n",
    "coocs_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c50b1a84-5725-49e2-a779-488efc12ecfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's compute cooccurrence counts in the corpus.\n",
    "window=2\n",
    "skip=0 # defines distance if skipgram\n",
    "ngrams = [ skipgrams(sent, window, skip) for sent in corpus ]\n",
    "ngram_counts = NgramCounter(ngrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "54f53a78-a7fb-4e32-a25e-c98242b61cbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('ars', 'amatorius', 2), ('ars', 'populus', 1), ('ars', 'cieo', 1), ('ars', 'levis', 1), ('ars', 'rego', 2), ('ars', 'ferus', 2), ('ars', 'nec', 9), ('ars', 'sum', 129), ('ars', 'careo', 4), ('ars', 'excutio', 1)]\n"
     ]
    }
   ],
   "source": [
    "# get triples (target, feature, freq) from the ngram_counts\n",
    "coocs = []\n",
    "for node, freqs in ngram_counts[window].items(): # dict_items([(('ars',), FreqDist({'et': 149, 'sum': 129, ...}}))])\n",
    "    #print(word)\n",
    "    for cooc, freq  in freqs.items():\n",
    "        triple = (node[0], cooc, freq) # node term, cooc, freq\n",
    "        coocs.append(triple)\n",
    "print(coocs[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "670d8d23-a5ab-428b-b6d7-c194071e93f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>feature</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ars</td>\n",
       "      <td>amatorius</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ars</td>\n",
       "      <td>populus</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ars</td>\n",
       "      <td>cieo</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ars</td>\n",
       "      <td>levis</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ars</td>\n",
       "      <td>rego</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  target    feature  score\n",
       "0    ars  amatorius      2\n",
       "1    ars    populus      1\n",
       "2    ars       cieo      1\n",
       "3    ars      levis      1\n",
       "4    ars       rego      2"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convert cooc counts to matrix\n",
    "coocs_df = pd.DataFrame(coocs, columns=[\"target\", \"feature\", \"score\"])\n",
    "coocs_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d7a8ee6c-c3d0-4df8-9491-41bc0171cc32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distributional Semantic Model with 192519 rows x 196956 columns\n",
      "* raw co-occurrence matrix M available\n",
      "  - sparse matrix with 2779.5k / 37.9G nonzero entries (fill rate = 0.01%)\n",
      "  - in canonical format\n",
      "  - known to be non-negative\n",
      "  - sample size of underlying corpus: 7832.7k tokens\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# create dsm matrix\n",
    "VObj = wordspace.dsm(target=StrVector(coocs_df[\"target\"]),\n",
    "                     feature=StrVector(coocs_df[\"feature\"]),\n",
    "                     score=IntVector(coocs_df[\"score\"]),\n",
    "                     raw_freq=True) \n",
    "print(VObj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d6374411-fea9-4008-98a9-bcf596a34f42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'score': 'simple-ll', 'transform': 'log', 'normalize': True, 'method': 'euclidean', 'reduce_method': 'svd', 'reduce_n': 300}\n",
      "Distributional Semantic Model with 192519 rows x 196956 columns\n",
      "* raw co-occurrence matrix M available\n",
      "  - sparse matrix with 2779.5k / 37.9G nonzero entries (fill rate = 0.01%)\n",
      "  - in canonical format\n",
      "  - known to be non-negative\n",
      "  - sample size of underlying corpus: 7832.7k tokens\n",
      "* scored matrix S available\n",
      "  - sparse matrix with 2500.3k / 37.9G nonzero entries (fill rate = 0.01%)\n",
      "  - in canonical format\n",
      "  - known to be non-negative\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#default options\n",
    "config = dict(score=\"simple-ll\", transform=\"log\", normalize=True, method=\"euclidean\", reduce_method=\"svd\", reduce_n=300)\n",
    "print(config)\n",
    "VObj_weighted = wordspace.dsm_score(VObj, score=config[\"score\"], transform=config[\"transform\"], normalize=config[\"normalize\"], method=config[\"method\"])\n",
    "print(VObj_weighted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8e9de87d-ac20-4ba9-b744-9d42ca6298e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.03863485012165868\n"
     ]
    }
   ],
   "source": [
    "VObj_weighted_reduced = wordspace.dsm_projection(VObj_weighted, method=config[\"reduce_method\"], n=config[\"reduce_n\"])\n",
    "print(VObj_weighted_reduced[0]) # takes around 5 mins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "21c16df6-bbfe-4401-b6e3-a03685727807",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save matrices to disk\n",
    "matrices = [VObj_weighted, VObj_weighted_reduced]\n",
    "import pickle\n",
    "f = open('dsm_matrices.model', 'wb')\n",
    "pickle.dump(matrices,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71fe7a69-78f1-47be-aff6-32717540754c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get similar from unreduced space\n",
    "[ print(term, \"\\n\", wordspace.nearest_neighbours(VObj_weighted, term, n=10)) for term in socio_political_terms ]\n",
    "# get similar from reduced space\n",
    "[ print(term, \"\\n\", wordspace.nearest_neighbours(VObj_weighted_reduced, term, n=10)) for term in socio_political_terms ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "64e9dfc1-b27a-4284-97e5-90d619dd8937",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dictionary that maps a time interval with the list of sentences of texts in that time interval\"\n",
    "time2corpus = dict()\n",
    "\n",
    "# I loop over all time intervals:\n",
    "for t in range(n_intervals+1):\n",
    "    files_corpus_t = metadata_df.loc[metadata_df['time_interval'] == intervals[t]]\n",
    "    #print(\"1:\",files_corpus_t, type(files_corpus_t))\n",
    "    corpus_t = list()\n",
    "    for index, df_line in files_corpus_t.iterrows():\n",
    "        #print(\"line:\",df_line['id'], df_line['time_interval'])\n",
    "        sign = \"+\"\n",
    "        #print(df_line['date'])\n",
    "        if df_line['date'] < 0:\n",
    "            sign = \"-\"\n",
    "        #print(\"date:\", convert_dates(sign, abs(df_line['date'])))\n",
    "        file_name = 'lat_'+str(convert_dates(sign, abs(df_line['date'])))+\"_\"+str(df_line['id'])+'.txt'\n",
    "        #print(\"3:\",file_name)\n",
    "        #KN: missing files\n",
    "        if os.path.isfile(os.path.join(dir_in, file_name)):\n",
    "            file = open(os.path.join(dir_in, file_name), 'r')\n",
    "            sentences_this_file = list()\n",
    "            while True:\n",
    "                line = file.readline().strip()\n",
    "                if line != \"\":\n",
    "                    #sentences_this_file.append(line.split(\" \"))\n",
    "                    #sentences_this_file.append([token for token in line.split(\" \") if token not in punctuation])\n",
    "                    corpus_t.append([token.lower() for token in line.split(\" \") if token not in punctuation]) #KN: tolower\n",
    "                # if line is empty end of file is reached\n",
    "                if not line:\n",
    "                    break\n",
    "            file.close()\n",
    "        #corpus_t.append(sentences_this_file)\n",
    "    #corpus_t1\n",
    "    #print(len(corpus_t1[0]))\n",
    "    time2corpus[t] = corpus_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e766e991-54ae-48ce-b4c2-5a198364526b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['dominus',\n",
       "  'iesus',\n",
       "  'antequam',\n",
       "  'in',\n",
       "  'coelum',\n",
       "  'ascendo',\n",
       "  'suus',\n",
       "  'discipulus',\n",
       "  'mandatum',\n",
       "  'confero',\n",
       "  'nuntio',\n",
       "  'evangelium',\n",
       "  'cunctus',\n",
       "  'homo',\n",
       "  'omnesque',\n",
       "  'populus',\n",
       "  'baptizo',\n",
       "  'eo',\n",
       "  'in',\n",
       "  'mundum',\n",
       "  'universus',\n",
       "  'praedico#1',\n",
       "  'evangelium',\n",
       "  'omnis',\n",
       "  'creatura'],\n",
       " ['qui',\n",
       "  'credo',\n",
       "  'et',\n",
       "  'baptizo',\n",
       "  'sum',\n",
       "  'salvus',\n",
       "  'sum',\n",
       "  'quis#2',\n",
       "  'verus',\n",
       "  'non',\n",
       "  'credo',\n",
       "  'condemno',\n",
       "  '(',\n",
       "  'mc',\n",
       "  '-',\n",
       "  ')',\n",
       "  'data',\n",
       "  'sum',\n",
       "  'ego',\n",
       "  'omnis',\n",
       "  'potestas',\n",
       "  'in',\n",
       "  'caelum',\n",
       "  'et',\n",
       "  'in',\n",
       "  'terra']]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time2corpus[4][0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "fa062127-eb0f-445c-b64c-db4d03edc813",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: None, 1: None, 2: None, 3: None, 4: None, 5: None}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "window=2\n",
    "skip=0 # defines distance if skipgram\n",
    "# DSM options\n",
    "config = dict(score=\"simple-ll\", transform=\"log\", normalize=True, method=\"euclidean\", reduce_method=\"svd\", reduce_n=300)\n",
    "models = dict.fromkeys(range(len(time2corpus)))\n",
    "models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9da0497b-1e54-4a02-9b68-f9b6c17d7254",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "for t in range(len(time2corpus)):\n",
    "    if len(time2corpus[t])>0:\n",
    "        print(\"Building model for t = \", t, \"\\n\")\n",
    "        \n",
    "        print(\"Retrieving coocurrence counts for t = \", t, \"\\n\")\n",
    "        #prepare coocs\n",
    "        ngrams = [ skipgrams(sent, window, skip) for sent in time2corpus[t] ]\n",
    "        ngram_counts = NgramCounter(ngrams)\n",
    "        \n",
    "        print(\"Retrieving (target, feature, freq) from the ngram_counts for t = \", t, \"\\n\")\n",
    "        # get triples (target, feature, freq) from the ngram_counts\n",
    "        coocs = []\n",
    "        for node, freqs in ngram_counts[window].items(): # dict_items([(('ars',), FreqDist({'et': 149, 'sum': 129, ...}}))])\n",
    "            for cooc, freq  in freqs.items():\n",
    "                triple = (node[0], cooc, freq) # node term, cooc, freq\n",
    "                coocs.append(triple)\n",
    "        \n",
    "        coocs_df = pd.DataFrame(coocs, columns=[\"target\", \"feature\", \"score\"])\n",
    "        #coocs_df.head()\n",
    "        \n",
    "        print(\"Building DSM matrices for t = \", t, \"\\n\")\n",
    "\n",
    "        # create dms matrix\n",
    "        VObj = wordspace.dsm(target=StrVector(coocs_df[\"target\"]),\n",
    "                             feature=StrVector(coocs_df[\"feature\"]),\n",
    "                             score=IntVector(coocs_df[\"score\"]),\n",
    "                             raw_freq=True) \n",
    "        VObj_weighted = wordspace.dsm_score(VObj, score=config[\"score\"], transform=config[\"transform\"], normalize=config[\"normalize\"], method=config[\"method\"])\n",
    "        VObj_weighted_reduced = wordspace.dsm_projection(VObj_weighted, method=config[\"reduce_method\"], n=config[\"reduce_n\"])\n",
    "        \n",
    "        print(\"Saving models for t = \", t, \"\\n\")\n",
    "        models[t] = (VObj_weighted, VObj_weighted_reduced)\n",
    "        \n",
    "end = time.time()\n",
    "print(\"It has taken\", round(end - start), \"seconds, or \", round((end - start)/60), \"minutes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "3de12e25-65ed-4a36-8007-1bad7a404f45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save diachronic models to disk\n",
    "import pickle\n",
    "f = open('dsm_matrices_time.model', 'wb')\n",
    "pickle.dump(models,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "13a0222b-0e87-4293-b8a8-f964f5399104",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load models from disk\n",
    "#import pickle\n",
    "#f = open('dsm_matrices_time.model', 'rb')\n",
    "#models = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3bf6c5d-888b-4841-968c-45437046642c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get similarity counts\n",
    "for t, model in models.items():\n",
    "    if model is not None:\n",
    "        print(\"Similarities for t =\", t)\n",
    "        print(\"Unreduced matrix for t =\", t)    \n",
    "        [ print(term, \"\\n\", wordspace.nearest_neighbours(model[0], term, n=10) \n",
    "                if term in list(models[t][0].rx2('rows').rx2(\"term\")) else None)\n",
    "         for term in socio_political_terms ]\n",
    "        print(\"Reduced matrix for t =\", t)\n",
    "        [ print(term, \"\\n\", wordspace.nearest_neighbours(model[1], term, n=10)  \n",
    "                if term in list(models[t][0].rx2('rows').rx2(\"term\")) else None) \n",
    "         for term in socio_political_terms ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52862de7-95a8-42b0-a6a4-ff6ad719050c",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Experimenting with DSM options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "631b2810-e93d-4396-a772-5db7dfaf571c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "from nltk.util import everygrams, skipgrams\n",
    "from nltk.util import ngrams\n",
    "windows = range(2,5+1)\n",
    "scores = [ \"MI\", \"log-likelihood\", \"simple-ll\", \"t-score\", \"chi-squared\", \"z-score\", \"tf.idf\" ]\n",
    "transforms = [ \"none\", \"log\", \"root\", \"sigmoid\" ]\n",
    "proj_methods = [ \"svd\", \"rsvd\" ]\n",
    "threshold = [0, 3, 5, 10]\n",
    "vectors = [50, 100, 300]\n",
    "periods = [3] # test on 1 slice\n",
    "normalizes = [ True, False ]\n",
    "configs = []\n",
    "[ configs.append({\"window\":x[0], \"score\":x[1], \"transform\":x[2], \n",
    "                  \"min_f\":x[3], \"size\":x[4], \"period\":x[5], \"method\":x[6], \"normalize\":x[7] }) \n",
    " for x in product(windows, scores, transforms, threshold, vectors, periods, proj_methods, normalizes) ]\n",
    "configs = []\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e224f92-d7f0-4212-be1f-874b297e0a75",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Splitting the corpus\n",
    "The corpus is splitted into slices, each covering `size_interval` years."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a518dadf-71f1-4f06-971d-153cb8b3aafe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "retrieving the subcorpus for interval  -450\n",
      "retrieving the subcorpus for interval  0\n",
      "retrieving the subcorpus for interval  450\n"
     ]
    }
   ],
   "source": [
    "# dictionary that maps a time interval with the list of sentences of texts in that time interval\"\n",
    "time2corpus = dict()\n",
    "\n",
    "# I loop over all time intervals:\n",
    "#for t in range(n_intervals+1): # remove redundant 900 interval\n",
    "for t in range(n_intervals):\n",
    "    files_corpus_t = list(corpus_subset.loc[corpus_subset['time_interval'] == intervals[t]][\"file\"])\n",
    "    print(\"retrieving the subcorpus for interval \", intervals[t])\n",
    "    sents = latinise.sents(fileids=files_corpus_t)\n",
    "    sents_clean = list()\n",
    "    for sent in sents:\n",
    "        sents_clean.append( [ token.lower()  for token in sent if token not in punctuation and token != ''  ] )\n",
    "    time2corpus[t] = sents_clean"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82b95e1d-1fcb-4804-8729-6fe167912953",
   "metadata": {},
   "source": [
    "The `time2corpus` variable is a dictionary with time slices as keys. Each item is a list of sentences, each being a list of lemmas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ab1dd244-8332-44fb-acfa-53a3e5bf3714",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary keys are: [0, 1, 2]\n",
      "First 3 sentences from the 3rd corpus slice are:  [['obsculta', 'o', 'filius', 'praeceptum', 'magister', 'et', 'inclino', 'auris', 'cor', 'tuus', 'et', 'admonitio', 'pius', 'pater', 'libet', 'excipe', 'et', 'efficaciter', 'comple', 'ut', 'ad', 'is', 'per', 'oboedientia', 'labor', 'redeo', 'ab', 'quo', 'per', 'inoboedientia', 'desidia', 'recedo'], ['ad', 'tu', 'ergo', 'nunc', 'ego', 'sermo', 'dirigo', 'quisquis', 'abrenuntio', 'proprius', 'voluntas', 'dominus', 'christus', 'verus', 'rex', 'militaturus', 'oboedientia', 'fortis', 'atque', 'praeclarus', 'arma', 'sumo']]\n"
     ]
    }
   ],
   "source": [
    "print(f'Dictionary keys are: { [ period for period in time2corpus.keys()] }')\n",
    "print('First 3 sentences from the 3rd corpus slice are: ', time2corpus[2][0:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2b46da45-2fa8-46f5-a5f5-66db3378428a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['obsculta',\n",
       "  'o',\n",
       "  'filius',\n",
       "  'praeceptum',\n",
       "  'magister',\n",
       "  'et',\n",
       "  'inclino',\n",
       "  'auris',\n",
       "  'cor',\n",
       "  'tuus',\n",
       "  'et',\n",
       "  'admonitio',\n",
       "  'pius',\n",
       "  'pater',\n",
       "  'libet',\n",
       "  'excipe',\n",
       "  'et',\n",
       "  'efficaciter',\n",
       "  'comple',\n",
       "  'ut',\n",
       "  'ad',\n",
       "  'is',\n",
       "  'per',\n",
       "  'oboedientia',\n",
       "  'labor',\n",
       "  'redeo',\n",
       "  'ab',\n",
       "  'quo',\n",
       "  'per',\n",
       "  'inoboedientia',\n",
       "  'desidia',\n",
       "  'recedo']]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time2corpus[2][0:1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7491b5e4-38a8-4a51-8d8f-9fc049759e0f",
   "metadata": {},
   "source": [
    "# OLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d27fbc09-5085-4149-bf51-747cd6ea9060",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "from nltk.util import everygrams, skipgrams\n",
    "from nltk.util import ngrams\n",
    "windows = range(2,5+1)\n",
    "scores = [ \"MI\", \"log-likelihood\", \"simple-ll\", \"t-score\", \"chi-squared\", \"z-score\", \"tf.idf\" ]\n",
    "transforms = [ \"none\", \"log\", \"root\", \"sigmoid\" ]\n",
    "proj_methods = [ \"svd\", \"rsvd\" ]\n",
    "threshold = [0, 3, 5, 10]\n",
    "vectors = [50, 100, 300]\n",
    "periods = [3] # test on 1 slice\n",
    "normalizes = [ True, False ]\n",
    "configs = []\n",
    "[ configs.append({\"window\":x[0], \"score\":x[1], \"transform\":x[2], \n",
    "                  \"min_f\":x[3], \"size\":x[4], \"period\":x[5], \"method\":x[6], \"normalize\":x[7] }) \n",
    " for x in product(windows, scores, transforms, threshold, vectors, periods, proj_methods, normalizes) ]\n",
    "configs = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bbdd7954-44e7-4bf6-869a-d053895e26b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: None, 1: None, 2: None, 3: None, 4: None, 5: None}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dict for ngram counts {period: {win1, win2...}}\n",
    "ngram_counts_all = dict.fromkeys( k for k, v in time2corpus.items() )\n",
    "ngram_counts_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1bd48c17-d648-4575-81b1-f09f89e93c8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving coocurrence counts for t =  3 , windows= range(2, 6) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for period in periods:\n",
    "    print(\"Retrieving coocurrence counts for t = \", period, \", windows=\", windows, \"\\n\")\n",
    "    #prepare coocs\n",
    "    skip = max(windows) - 2\n",
    "    ngrams = [ skipgrams(sent, 2, skip) for sent in time2corpus[period] ]\n",
    "    len(ngrams)\n",
    "    #print(ngrams)\n",
    "    ngram_counts = NgramCounter(ngrams)\n",
    "    len(ngram_counts)\n",
    "    ngram_counts_all[period] = ngram_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3403c6e2-6862-4067-ab7e-5eb15637ad54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: None,\n",
       " 1: None,\n",
       " 2: None,\n",
       " 3: <nltk.lm.counter.NgramCounter at 0x7f5916be0550>,\n",
       " 4: None,\n",
       " 5: None}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ngram_counts_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "500d7520-115f-474d-bb38-8c2d2a71d228",
   "metadata": {},
   "outputs": [],
   "source": [
    "#[ a for a in ngram_counts_all[3] ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "314881df-97a2-41c8-b809-78ef00632c13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It has taken 0 seconds, or  0 minutes\n"
     ]
    }
   ],
   "source": [
    "models = list()\n",
    "\n",
    "start = time.time()\n",
    "for config in configs:\n",
    "    print(\"model configuration: \", config)\n",
    "    if len(time2corpus[config['period']]) > 0:        \n",
    "        \n",
    "        print(\"Building model for t = \", config['period'], \"\\n\")\n",
    "        \n",
    "        print(\"Retrieving (target, feature, freq) from the ngram_counts for t = \", config['period'], \"\\n\")\n",
    "        # get triples (target, feature, freq) from the ngram_counts\n",
    "        coocs = []\n",
    "        ngram_counts = ngram_counts_all[config['period']]\n",
    "        for node, freqs in ngram_counts[config['period']].items(): # dict_items([(('ars',), FreqDist({'et': 149, 'sum': 129, ...}}))])\n",
    "            for cooc, freq  in freqs.items():\n",
    "                triple = (node[0], cooc, freq) # node term, cooc, freq\n",
    "                coocs.append(triple)\n",
    "        print(coocs)\n",
    "        coocs_df = pd.DataFrame(coocs, columns=[\"target\", \"feature\", \"score\"])\n",
    "        print(coocs_df.head())\n",
    "        \n",
    "        print(\"Building DSM matrices for t = \", config['period'], \"\\n\")\n",
    "\n",
    "        # create dms matrix\n",
    "        VObj = wordspace.dsm(target=StrVector(coocs_df[\"target\"]),\n",
    "                             feature=StrVector(coocs_df[\"feature\"]),\n",
    "                             score=IntVector(coocs_df[\"score\"]),\n",
    "                             raw_freq=True) \n",
    "        \n",
    "        VObj_weighted = wordspace.dsm_score(VObj, score=config[\"score\"], transform=config[\"transform\"],\n",
    "                                            normalize=config[\"normalize\"], method=config[\"method\"])\n",
    "        VObj_weighted_reduced = wordspace.dsm_projection(VObj_weighted, method=config[\"method\"], n=config[\"min_f\"])\n",
    "        \n",
    "        print(\"Saving models for t = \", config['period'], \"\\n\")\n",
    "        models.append((config, VObj_weighted, VObj_weighted_reduced))\n",
    "        \n",
    "end = time.time()\n",
    "print(\"It has taken\", round(end - start), \"seconds, or \", round((end - start)/60), \"minutes\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b275dfe-e346-4752-afaa-1a53c9ada22c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "toc-autonumbering": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
