{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ca6207f0-062b-41fd-afe5-e144af88339f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: rpy2 in /media/HOME_DATA/miniconda3/envs/lvlt22/lib/python3.10/site-packages (3.5.3)\n",
      "Requirement already satisfied: jinja2 in /media/HOME_DATA/miniconda3/envs/lvlt22/lib/python3.10/site-packages (from rpy2) (3.1.2)\n",
      "Requirement already satisfied: cffi>=1.10.0 in /media/HOME_DATA/miniconda3/envs/lvlt22/lib/python3.10/site-packages (from rpy2) (1.15.1)\n",
      "Requirement already satisfied: pytz in /media/HOME_DATA/miniconda3/envs/lvlt22/lib/python3.10/site-packages (from rpy2) (2022.1)\n",
      "Requirement already satisfied: tzlocal in /media/HOME_DATA/miniconda3/envs/lvlt22/lib/python3.10/site-packages (from rpy2) (4.2)\n",
      "Requirement already satisfied: pycparser in /media/HOME_DATA/miniconda3/envs/lvlt22/lib/python3.10/site-packages (from cffi>=1.10.0->rpy2) (2.21)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /media/HOME_DATA/miniconda3/envs/lvlt22/lib/python3.10/site-packages (from jinja2->rpy2) (2.1.1)\n",
      "Requirement already satisfied: pytz-deprecation-shim in /media/HOME_DATA/miniconda3/envs/lvlt22/lib/python3.10/site-packages (from tzlocal->rpy2) (0.1.0.post0)\n",
      "Requirement already satisfied: tzdata in /media/HOME_DATA/miniconda3/envs/lvlt22/lib/python3.10/site-packages (from pytz-deprecation-shim->tzlocal->rpy2) (2022.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install rpy2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6ce0818e-6f22-4b36-bcde-524c4ff170c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import time\n",
    "from nltk.util import skipgrams\n",
    "from nltk.lm import NgramCounter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "52afaceb-dfcc-49eb-83a4-554e64f3ced2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "R[write to console]: Instalowanie pakietu w ‘/home/krzys/R/x86_64-pc-linux-gnu-library/4.1’\n",
      "(ponieważ ‘lib’ nie jest określony)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"/home/krzys/R/x86_64-pc-linux-gnu-library/4.1\"\n",
      "[2] \"/usr/local/lib/R/site-library\"                \n",
      "[3] \"/usr/lib/R/site-library\"                      \n",
      "[4] \"/usr/lib/R/library\"                           \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "R[write to console]: próbowanie adresu URL 'https://cloud.r-project.org/src/contrib/wordspace_0.2-7.tar.gz'\n",
      "\n",
      "R[write to console]: Content type 'application/x-gzip'\n",
      "R[write to console]:  length 1840058 bytes (1.8 MB)\n",
      "\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: =\n",
      "R[write to console]: \n",
      "\n",
      "R[write to console]: downloaded 1.8 MB\n",
      "\n",
      "\n",
      "* installing *source* package ‘wordspace’ ...\n",
      "** package ‘wordspace’ successfully unpacked and MD5 sums checked\n",
      "** using staged installation\n",
      "** libs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "g++ -std=gnu++14 -I\"/usr/share/R/include\" -DNDEBUG  -I'/home/krzys/R/x86_64-pc-linux-gnu-library/4.1/Rcpp/include'   -fopenmp -fpic  -g -O2 -ffile-prefix-map=/build/r-base-4A2Reg/r-base-4.1.2=. -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -g  -c RcppExports.cpp -o RcppExports.o\n",
      "g++ -std=gnu++14 -I\"/usr/share/R/include\" -DNDEBUG  -I'/home/krzys/R/x86_64-pc-linux-gnu-library/4.1/Rcpp/include'   -fopenmp -fpic  -g -O2 -ffile-prefix-map=/build/r-base-4A2Reg/r-base-4.1.2=. -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -g  -c wordspace.cpp -o wordspace.o\n",
      "g++ -std=gnu++14 -shared -L/usr/lib/R/lib -Wl,-Bsymbolic-functions -flto=auto -ffat-lto-objects -flto=auto -Wl,-z,relro -o wordspace.so RcppExports.o wordspace.o -fopenmp -L/usr/lib/R/lib -lR\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "installing to /home/krzys/R/x86_64-pc-linux-gnu-library/4.1/00LOCK-wordspace/00new/wordspace/libs\n",
      "** R\n",
      "** data\n",
      "*** moving datasets to lazyload DB\n",
      "** inst\n",
      "** byte-compile and prepare package for lazy loading\n",
      "** help\n",
      "*** installing help indices\n",
      "** building package indices\n",
      "** installing vignettes\n",
      "** testing if installed package can be loaded from temporary location\n",
      "** checking absolute paths in shared objects and dynamic libraries\n",
      "** testing if installed package can be loaded from final location\n",
      "** testing if installed package keeps a record of temporary installation path\n",
      "* DONE (wordspace)\n",
      "R[write to console]: \n",
      "\n",
      "R[write to console]: \n",
      "R[write to console]: The downloaded source packages are in\n",
      "\t‘/tmp/RtmpYf2CJN/downloaded_packages’\n",
      "R[write to console]: \n",
      "R[write to console]: \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import rpy2.robjects as robjects\n",
    "from rpy2.robjects.packages import importr\n",
    "import rpy2.robjects.packages as rpackages\n",
    "from rpy2.robjects import StrVector, IntVector\n",
    "utils = rpackages.importr('utils')\n",
    "utils.chooseCRANmirror(ind=1) # select the first mirror in the list\n",
    "r_libs = '/home/krzys/R/x86_64-pc-linux-gnu-library/4.1'# R libs\n",
    "base = importr('base')\n",
    "print(base._libPaths())\n",
    "utils.install_packages(\"wordspace\")\n",
    "wordspace = importr(\"wordspace\", lib_loc=r_libs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a8fd273c-74ec-4992-8d5c-23f107da68fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define terms we're interested in\n",
    "socio_political_terms = [\"civitas\", \"consilium\", \"consul\", \"dux\", \"gens\", \"hostis\", \"imperator\", \"jus\", \"labor\", \"natio\", \"nobilitas\", \"pontifex\", \"pontificium\", \"populus\", \"potestas\", \"regnum\", \"senatus\", \"sodes\", \"urbs\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e627474f-b747-4166-a76a-c1e4b4bb59ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-450\n",
      "2005\n",
      "[-450, 50, 550, 1050, 1550, 2050]\n",
      "0\n",
      "range(-450, 50)\n",
      "19      -9\n",
      "34     -49\n",
      "38      49\n",
      "39     -45\n",
      "42     -49\n",
      "      ... \n",
      "635   -149\n",
      "638   -107\n",
      "642    -37\n",
      "643    -37\n",
      "649   -229\n",
      "Name: date, Length: 91, dtype: int64\n",
      "1\n",
      "range(50, 550)\n",
      "18     382\n",
      "20     524\n",
      "23     399\n",
      "24     391\n",
      "37     158\n",
      "      ... \n",
      "683    116\n",
      "684    116\n",
      "685    116\n",
      "686    116\n",
      "687    533\n",
      "Name: date, Length: 236, dtype: int64\n",
      "2\n",
      "range(550, 1050)\n",
      "102    800\n",
      "104    800\n",
      "105    800\n",
      "106    800\n",
      "107    800\n",
      "      ... \n",
      "601    833\n",
      "609    598\n",
      "628    950\n",
      "634    550\n",
      "636    550\n",
      "Name: date, Length: 66, dtype: int64\n",
      "3\n",
      "range(1050, 1550)\n",
      "21     1254\n",
      "22     1254\n",
      "25     1224\n",
      "26     1150\n",
      "28     1517\n",
      "       ... \n",
      "624    1067\n",
      "625    1110\n",
      "626    1110\n",
      "644    1501\n",
      "646    1483\n",
      "Name: date, Length: 178, dtype: int64\n",
      "4\n",
      "range(1550, 2050)\n",
      "27     2000\n",
      "33     1998\n",
      "35     2001\n",
      "36     1992\n",
      "220    1998\n",
      "       ... \n",
      "641    1685\n",
      "647    1550\n",
      "650    1555\n",
      "651    1684\n",
      "652    1684\n",
      "Name: date, Length: 99, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# prepare metadata\n",
    "dir_in = os.path.join(\"/home/krzys/Kod/streamlit/voces/data/corpora/latinise_IT_lemmas/\")\n",
    "dir_in\n",
    "files = os.listdir(os.path.join(dir_in))\n",
    "files = [f for f in files[:] if \"IT\" in f]\n",
    "metadata_df = pd.read_csv(os.path.join(dir_in, 'latinise_metadata.csv'), sep = \",\")\n",
    "metadata_df = metadata_df[metadata_df['id'].str.startswith(\"IT\")]\n",
    "metadata_df.head()\n",
    "\n",
    "# split the corpus into subcorpora\n",
    "first_date = min(metadata_df.date)\n",
    "last_date = max(metadata_df.date)\n",
    "print(first_date)\n",
    "print(last_date)\n",
    "size_interval = 500\n",
    "n_intervals = round((last_date-first_date)/size_interval)\n",
    "n_intervals\n",
    "intervals = [None]*(n_intervals+1)\n",
    "for t in range(n_intervals+1):\n",
    "    #print(t)\n",
    "    if t == 0:\n",
    "        intervals[t] = first_date\n",
    "    else:\n",
    "        intervals[t] = intervals[t-1]+size_interval\n",
    "    #print(intervals[t])\n",
    "    \n",
    "print(intervals)\n",
    "\n",
    "metadata_df['time_interval'] = \"\"\n",
    "for t in range(len(intervals)-1):\n",
    "    print(t)\n",
    "    print(range(intervals[t],intervals[t+1]))\n",
    "    metadata_df_t = metadata_df.loc[metadata_df['date'].isin(range(intervals[t],intervals[t+1]))]\n",
    "    print(metadata_df_t.date)\n",
    "    metadata_df.loc[metadata_df['date'].isin(range(intervals[t],intervals[t+1])),'time_interval'] = intervals[t]\n",
    "metadata_df\n",
    "\n",
    "def convert_dates(sign, date0):\n",
    "\n",
    "    if sign == \"0\":\n",
    "        if date0 == 0:\n",
    "            final_date = \"+0000\"\n",
    "        elif date0 < 100:\n",
    "            final_date = \"+\" + \"00\" + str(date0)\n",
    "            #print(\"1-final_date\", final_date)\n",
    "        elif date0 < 1000:\n",
    "            final_date = \"+\" + \"0\" + str(date0)\n",
    "            #print(\"2-final_date\", final_date)\n",
    "        else:\n",
    "            final_date = \"+\" + str(date0)\n",
    "            #print(\"3-final_date\", final_date)\n",
    "    else:\n",
    "        if date0 == 0:\n",
    "            final_date = \"+0000\"\n",
    "        elif date0 < 100:\n",
    "            final_date = str(sign) + \"00\" + str(date0)\n",
    "            #print(\"1-final_date\", final_date)\n",
    "        elif date0 < 1000:\n",
    "            final_date = str(sign) + \"0\" + str(date0)\n",
    "            #print(\"2-final_date\", final_date)\n",
    "        else:\n",
    "            final_date = str(sign) + str(date0)\n",
    "            #print(\"3-final_date\", final_date)\n",
    "\n",
    "    if final_date.startswith(\"+\"):\n",
    "        final_date = final_date.replace(\"+\", \"\")\n",
    "    return final_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "aa3e7a7a-7bba-445c-89a7-2d703b0abe84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare the corpus\n",
    "punctuation = ['.', ',', '...', ';', ':', '?']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "801e12c9-3b85-4fa7-ab50-040e3db85165",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'metadata_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_115259/2759612327.py\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mcorpus\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mcorpus_filenames\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mfiles_corpus\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmetadata_df\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf_line\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfiles_corpus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterrows\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;31m#print(\"line:\",df_line['id'], df_line['time_interval'])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'metadata_df' is not defined"
     ]
    }
   ],
   "source": [
    "corpus = list()\n",
    "corpus_filenames = list()\n",
    "files_corpus = metadata_df\n",
    "for index, df_line in files_corpus.iterrows():\n",
    "    #print(\"line:\",df_line['id'], df_line['time_interval'])\n",
    "    sign = \"+\"\n",
    "    #print(df_line['date'])\n",
    "    if df_line['date'] < 0:\n",
    "        sign = \"-\"\n",
    "    #print(\"date:\", convert_dates(sign, abs(df_line['date'])))\n",
    "    file_name = 'lat_'+str(convert_dates(sign, abs(df_line['date'])))+\"_\"+str(df_line['id'])+'.txt'\n",
    "    #KN: missing files\n",
    "    if os.path.isfile(os.path.join(dir_in, file_name)):\n",
    "        file = open(os.path.join(dir_in, file_name), 'r')\n",
    "        sentences_this_file = list()\n",
    "        while True:\n",
    "            line = file.readline().strip()\n",
    "            if line != \"\":\n",
    "                corpus.append([token.lower() for token in line.split(\" \") if token not in punctuation]) #KN: lemmas to lowcase\n",
    "                corpus_filenames.append(file_name) #KN: track the file name for the sentence\n",
    "            # if line is empty end of file is reached\n",
    "            if not line:\n",
    "                break\n",
    "        file.close()\n",
    "        corpus.append(sentences_this_file)\n",
    "    else:\n",
    "        print(os.path.join(dir_in, file_name), \" doesn't exist\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c50b1a84-5725-49e2-a779-488efc12ecfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's compute cooccurrence counts in the corpus.\n",
    "window=2\n",
    "skip=0 # defines distance if skipgram\n",
    "ngrams = [ skipgrams(sent, window, skip) for sent in corpus ]\n",
    "ngram_counts = NgramCounter(ngrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "54f53a78-a7fb-4e32-a25e-c98242b61cbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('ars', 'amatorius', 2), ('ars', 'populus', 1), ('ars', 'cieo', 1), ('ars', 'levis', 1), ('ars', 'rego', 2), ('ars', 'ferus', 2), ('ars', 'nec', 9), ('ars', 'sum', 129), ('ars', 'careo', 4), ('ars', 'excutio', 1)]\n"
     ]
    }
   ],
   "source": [
    "# get triples (target, feature, freq) from the ngram_counts\n",
    "coocs = []\n",
    "for node, freqs in ngram_counts[window].items(): # dict_items([(('ars',), FreqDist({'et': 149, 'sum': 129, ...}}))])\n",
    "    #print(word)\n",
    "    for cooc, freq  in freqs.items():\n",
    "        triple = (node[0], cooc, freq) # node term, cooc, freq\n",
    "        coocs.append(triple)\n",
    "print(coocs[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "670d8d23-a5ab-428b-b6d7-c194071e93f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>feature</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ars</td>\n",
       "      <td>amatorius</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ars</td>\n",
       "      <td>populus</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ars</td>\n",
       "      <td>cieo</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ars</td>\n",
       "      <td>levis</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ars</td>\n",
       "      <td>rego</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  target    feature  score\n",
       "0    ars  amatorius      2\n",
       "1    ars    populus      1\n",
       "2    ars       cieo      1\n",
       "3    ars      levis      1\n",
       "4    ars       rego      2"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convert cooc counts to matrix\n",
    "coocs_df = pd.DataFrame(coocs, columns=[\"target\", \"feature\", \"score\"])\n",
    "coocs_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d7a8ee6c-c3d0-4df8-9491-41bc0171cc32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distributional Semantic Model with 192519 rows x 196956 columns\n",
      "* raw co-occurrence matrix M available\n",
      "  - sparse matrix with 2779.5k / 37.9G nonzero entries (fill rate = 0.01%)\n",
      "  - in canonical format\n",
      "  - known to be non-negative\n",
      "  - sample size of underlying corpus: 7832.7k tokens\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# create dsm matrix\n",
    "VObj = wordspace.dsm(target=StrVector(coocs_df[\"target\"]),\n",
    "                     feature=StrVector(coocs_df[\"feature\"]),\n",
    "                     score=IntVector(coocs_df[\"score\"]),\n",
    "                     raw_freq=True) \n",
    "print(VObj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d6374411-fea9-4008-98a9-bcf596a34f42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'score': 'simple-ll', 'transform': 'log', 'normalize': True, 'method': 'euclidean', 'reduce_method': 'svd', 'reduce_n': 300}\n",
      "Distributional Semantic Model with 192519 rows x 196956 columns\n",
      "* raw co-occurrence matrix M available\n",
      "  - sparse matrix with 2779.5k / 37.9G nonzero entries (fill rate = 0.01%)\n",
      "  - in canonical format\n",
      "  - known to be non-negative\n",
      "  - sample size of underlying corpus: 7832.7k tokens\n",
      "* scored matrix S available\n",
      "  - sparse matrix with 2500.3k / 37.9G nonzero entries (fill rate = 0.01%)\n",
      "  - in canonical format\n",
      "  - known to be non-negative\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#default options\n",
    "config = dict(score=\"simple-ll\", transform=\"log\", normalize=True, method=\"euclidean\", reduce_method=\"svd\", reduce_n=300)\n",
    "print(config)\n",
    "VObj_weighted = wordspace.dsm_score(VObj, score=config[\"score\"], transform=config[\"transform\"], normalize=config[\"normalize\"], method=config[\"method\"])\n",
    "print(VObj_weighted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8e9de87d-ac20-4ba9-b744-9d42ca6298e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.03863485012165868\n"
     ]
    }
   ],
   "source": [
    "VObj_weighted_reduced = wordspace.dsm_projection(VObj_weighted, method=config[\"reduce_method\"], n=config[\"reduce_n\"])\n",
    "print(VObj_weighted_reduced[0]) # takes around 5 mins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "21c16df6-bbfe-4401-b6e3-a03685727807",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save matrices to disk\n",
    "matrices = [VObj_weighted, VObj_weighted_reduced]\n",
    "import pickle\n",
    "f = open('dsm_matrices.model', 'wb')\n",
    "pickle.dump(matrices,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71fe7a69-78f1-47be-aff6-32717540754c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get similar from unreduced space\n",
    "[ print(term, \"\\n\", wordspace.nearest_neighbours(VObj_weighted, term, n=10)) for term in socio_political_terms ]\n",
    "# get similar from reduced space\n",
    "[ print(term, \"\\n\", wordspace.nearest_neighbours(VObj_weighted_reduced, term, n=10)) for term in socio_political_terms ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "64e9dfc1-b27a-4284-97e5-90d619dd8937",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dictionary that maps a time interval with the list of sentences of texts in that time interval\"\n",
    "time2corpus = dict()\n",
    "\n",
    "# I loop over all time intervals:\n",
    "for t in range(n_intervals+1):\n",
    "    files_corpus_t = metadata_df.loc[metadata_df['time_interval'] == intervals[t]]\n",
    "    #print(\"1:\",files_corpus_t, type(files_corpus_t))\n",
    "    corpus_t = list()\n",
    "    for index, df_line in files_corpus_t.iterrows():\n",
    "        #print(\"line:\",df_line['id'], df_line['time_interval'])\n",
    "        sign = \"+\"\n",
    "        #print(df_line['date'])\n",
    "        if df_line['date'] < 0:\n",
    "            sign = \"-\"\n",
    "        #print(\"date:\", convert_dates(sign, abs(df_line['date'])))\n",
    "        file_name = 'lat_'+str(convert_dates(sign, abs(df_line['date'])))+\"_\"+str(df_line['id'])+'.txt'\n",
    "        #print(\"3:\",file_name)\n",
    "        #KN: missing files\n",
    "        if os.path.isfile(os.path.join(dir_in, file_name)):\n",
    "            file = open(os.path.join(dir_in, file_name), 'r')\n",
    "            sentences_this_file = list()\n",
    "            while True:\n",
    "                line = file.readline().strip()\n",
    "                if line != \"\":\n",
    "                    #sentences_this_file.append(line.split(\" \"))\n",
    "                    #sentences_this_file.append([token for token in line.split(\" \") if token not in punctuation])\n",
    "                    corpus_t.append([token.lower() for token in line.split(\" \") if token not in punctuation]) #KN: tolower\n",
    "                # if line is empty end of file is reached\n",
    "                if not line:\n",
    "                    break\n",
    "            file.close()\n",
    "        #corpus_t.append(sentences_this_file)\n",
    "    #corpus_t1\n",
    "    #print(len(corpus_t1[0]))\n",
    "    time2corpus[t] = corpus_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e766e991-54ae-48ce-b4c2-5a198364526b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['dominus',\n",
       "  'iesus',\n",
       "  'antequam',\n",
       "  'in',\n",
       "  'coelum',\n",
       "  'ascendo',\n",
       "  'suus',\n",
       "  'discipulus',\n",
       "  'mandatum',\n",
       "  'confero',\n",
       "  'nuntio',\n",
       "  'evangelium',\n",
       "  'cunctus',\n",
       "  'homo',\n",
       "  'omnesque',\n",
       "  'populus',\n",
       "  'baptizo',\n",
       "  'eo',\n",
       "  'in',\n",
       "  'mundum',\n",
       "  'universus',\n",
       "  'praedico#1',\n",
       "  'evangelium',\n",
       "  'omnis',\n",
       "  'creatura'],\n",
       " ['qui',\n",
       "  'credo',\n",
       "  'et',\n",
       "  'baptizo',\n",
       "  'sum',\n",
       "  'salvus',\n",
       "  'sum',\n",
       "  'quis#2',\n",
       "  'verus',\n",
       "  'non',\n",
       "  'credo',\n",
       "  'condemno',\n",
       "  '(',\n",
       "  'mc',\n",
       "  '-',\n",
       "  ')',\n",
       "  'data',\n",
       "  'sum',\n",
       "  'ego',\n",
       "  'omnis',\n",
       "  'potestas',\n",
       "  'in',\n",
       "  'caelum',\n",
       "  'et',\n",
       "  'in',\n",
       "  'terra']]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time2corpus[4][0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "fa062127-eb0f-445c-b64c-db4d03edc813",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: None, 1: None, 2: None, 3: None, 4: None, 5: None}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "window=2\n",
    "skip=0 # defines distance if skipgram\n",
    "# DSM options\n",
    "config = dict(score=\"simple-ll\", transform=\"log\", normalize=True, method=\"euclidean\", reduce_method=\"svd\", reduce_n=300)\n",
    "models = dict.fromkeys(range(len(time2corpus)))\n",
    "models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9da0497b-1e54-4a02-9b68-f9b6c17d7254",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "for t in range(len(time2corpus)):\n",
    "    if len(time2corpus[t])>0:\n",
    "        print(\"Building model for t = \", t, \"\\n\")\n",
    "        \n",
    "        print(\"Retrieving coocurrence counts for t = \", t, \"\\n\")\n",
    "        #prepare coocs\n",
    "        ngrams = [ skipgrams(sent, window, skip) for sent in time2corpus[t] ]\n",
    "        ngram_counts = NgramCounter(ngrams)\n",
    "        \n",
    "        print(\"Retrieving (target, feature, freq) from the ngram_counts for t = \", t, \"\\n\")\n",
    "        # get triples (target, feature, freq) from the ngram_counts\n",
    "        coocs = []\n",
    "        for node, freqs in ngram_counts[window].items(): # dict_items([(('ars',), FreqDist({'et': 149, 'sum': 129, ...}}))])\n",
    "            for cooc, freq  in freqs.items():\n",
    "                triple = (node[0], cooc, freq) # node term, cooc, freq\n",
    "                coocs.append(triple)\n",
    "        \n",
    "        coocs_df = pd.DataFrame(coocs, columns=[\"target\", \"feature\", \"score\"])\n",
    "        #coocs_df.head()\n",
    "        \n",
    "        print(\"Building DSM matrices for t = \", t, \"\\n\")\n",
    "\n",
    "        # create dms matrix\n",
    "        VObj = wordspace.dsm(target=StrVector(coocs_df[\"target\"]),\n",
    "                             feature=StrVector(coocs_df[\"feature\"]),\n",
    "                             score=IntVector(coocs_df[\"score\"]),\n",
    "                             raw_freq=True) \n",
    "        VObj_weighted = wordspace.dsm_score(VObj, score=config[\"score\"], transform=config[\"transform\"], normalize=config[\"normalize\"], method=config[\"method\"])\n",
    "        VObj_weighted_reduced = wordspace.dsm_projection(VObj_weighted, method=config[\"reduce_method\"], n=config[\"reduce_n\"])\n",
    "        \n",
    "        print(\"Saving models for t = \", t, \"\\n\")\n",
    "        models[t] = (VObj_weighted, VObj_weighted_reduced)\n",
    "        \n",
    "end = time.time()\n",
    "print(\"It has taken\", round(end - start), \"seconds, or \", round((end - start)/60), \"minutes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "3de12e25-65ed-4a36-8007-1bad7a404f45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save diachronic models to disk\n",
    "import pickle\n",
    "f = open('dsm_matrices_time.model', 'wb')\n",
    "pickle.dump(models,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "13a0222b-0e87-4293-b8a8-f964f5399104",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load models from disk\n",
    "#import pickle\n",
    "#f = open('dsm_matrices_time.model', 'rb')\n",
    "#models = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3bf6c5d-888b-4841-968c-45437046642c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get similarity counts\n",
    "for t, model in models.items():\n",
    "    if model is not None:\n",
    "        print(\"Similarities for t =\", t)\n",
    "        print(\"Unreduced matrix for t =\", t)    \n",
    "        [ print(term, \"\\n\", wordspace.nearest_neighbours(model[0], term, n=10) \n",
    "                if term in list(models[t][0].rx2('rows').rx2(\"term\")) else None)\n",
    "         for term in socio_political_terms ]\n",
    "        print(\"Reduced matrix for t =\", t)\n",
    "        [ print(term, \"\\n\", wordspace.nearest_neighbours(model[1], term, n=10)  \n",
    "                if term in list(models[t][0].rx2('rows').rx2(\"term\")) else None) \n",
    "         for term in socio_political_terms ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52862de7-95a8-42b0-a6a4-ff6ad719050c",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Experimenting with DSM options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "631b2810-e93d-4396-a772-5db7dfaf571c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "from nltk.util import everygrams, skipgrams\n",
    "from nltk.util import ngrams\n",
    "windows = range(2,5+1)\n",
    "scores = [ \"MI\", \"log-likelihood\", \"simple-ll\", \"t-score\", \"chi-squared\", \"z-score\", \"tf.idf\" ]\n",
    "transforms = [ \"none\", \"log\", \"root\", \"sigmoid\" ]\n",
    "proj_methods = [ \"svd\", \"rsvd\" ]\n",
    "threshold = [0, 3, 5, 10]\n",
    "vectors = [50, 100, 300]\n",
    "periods = [3] # test on 1 slice\n",
    "normalizes = [ True, False ]\n",
    "configs = []\n",
    "[ configs.append({\"window\":x[0], \"score\":x[1], \"transform\":x[2], \n",
    "                  \"min_f\":x[3], \"size\":x[4], \"period\":x[5], \"method\":x[6], \"normalize\":x[7] }) \n",
    " for x in product(windows, scores, transforms, threshold, vectors, periods, proj_methods, normalizes) ]\n",
    "configs = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bbdd7954-44e7-4bf6-869a-d053895e26b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: None, 1: None, 2: None, 3: None, 4: None, 5: None}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dict for ngram counts {period: {win1, win2...}}\n",
    "ngram_counts_all = dict.fromkeys( k for k, v in time2corpus.items() )\n",
    "ngram_counts_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1bd48c17-d648-4575-81b1-f09f89e93c8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving coocurrence counts for t =  3 , windows= range(2, 6) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for period in periods:\n",
    "    print(\"Retrieving coocurrence counts for t = \", period, \", windows=\", windows, \"\\n\")\n",
    "    #prepare coocs\n",
    "    skip = max(windows) - 2\n",
    "    ngrams = [ skipgrams(sent, 2, skip) for sent in time2corpus[period] ]\n",
    "    len(ngrams)\n",
    "    #print(ngrams)\n",
    "    ngram_counts = NgramCounter(ngrams)\n",
    "    len(ngram_counts)\n",
    "    ngram_counts_all[period] = ngram_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3403c6e2-6862-4067-ab7e-5eb15637ad54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: None,\n",
       " 1: None,\n",
       " 2: None,\n",
       " 3: <nltk.lm.counter.NgramCounter at 0x7f5916be0550>,\n",
       " 4: None,\n",
       " 5: None}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ngram_counts_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "500d7520-115f-474d-bb38-8c2d2a71d228",
   "metadata": {},
   "outputs": [],
   "source": [
    "#[ a for a in ngram_counts_all[3] ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "314881df-97a2-41c8-b809-78ef00632c13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It has taken 0 seconds, or  0 minutes\n"
     ]
    }
   ],
   "source": [
    "models = list()\n",
    "\n",
    "start = time.time()\n",
    "for config in configs:\n",
    "    print(\"model configuration: \", config)\n",
    "    if len(time2corpus[config['period']]) > 0:        \n",
    "        \n",
    "        print(\"Building model for t = \", config['period'], \"\\n\")\n",
    "        \n",
    "        print(\"Retrieving (target, feature, freq) from the ngram_counts for t = \", config['period'], \"\\n\")\n",
    "        # get triples (target, feature, freq) from the ngram_counts\n",
    "        coocs = []\n",
    "        ngram_counts = ngram_counts_all[config['period']]\n",
    "        for node, freqs in ngram_counts[config['period']].items(): # dict_items([(('ars',), FreqDist({'et': 149, 'sum': 129, ...}}))])\n",
    "            for cooc, freq  in freqs.items():\n",
    "                triple = (node[0], cooc, freq) # node term, cooc, freq\n",
    "                coocs.append(triple)\n",
    "        print(coocs)\n",
    "        coocs_df = pd.DataFrame(coocs, columns=[\"target\", \"feature\", \"score\"])\n",
    "        print(coocs_df.head())\n",
    "        \n",
    "        print(\"Building DSM matrices for t = \", config['period'], \"\\n\")\n",
    "\n",
    "        # create dms matrix\n",
    "        VObj = wordspace.dsm(target=StrVector(coocs_df[\"target\"]),\n",
    "                             feature=StrVector(coocs_df[\"feature\"]),\n",
    "                             score=IntVector(coocs_df[\"score\"]),\n",
    "                             raw_freq=True) \n",
    "        \n",
    "        VObj_weighted = wordspace.dsm_score(VObj, score=config[\"score\"], transform=config[\"transform\"],\n",
    "                                            normalize=config[\"normalize\"], method=config[\"method\"])\n",
    "        VObj_weighted_reduced = wordspace.dsm_projection(VObj_weighted, method=config[\"method\"], n=config[\"min_f\"])\n",
    "        \n",
    "        print(\"Saving models for t = \", config['period'], \"\\n\")\n",
    "        models.append((config, VObj_weighted, VObj_weighted_reduced))\n",
    "        \n",
    "end = time.time()\n",
    "print(\"It has taken\", round(end - start), \"seconds, or \", round((end - start)/60), \"minutes\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b275dfe-e346-4752-afaa-1a53c9ada22c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "toc-autonumbering": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
